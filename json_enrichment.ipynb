{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPMS2CsSUJ7LUDnfPyWQ3gn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aithentic-dev/newspaper-clip-extraction/blob/main/json_enrichment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 1: Install dependencies ---\n",
        "!pip install azure-storage-file-share openai\n",
        "\n",
        "# --- Step 2: Imports ---\n",
        "import os, json\n",
        "from azure.storage.fileshare import ShareDirectoryClient, ShareFileClient\n",
        "from datetime import datetime\n",
        "from openai import OpenAI\n",
        "\n",
        "# --- Step 3: Configurations (using userdata) ---\n",
        "AZURE_CONN_STR = userdata.get(\"AZURE_CONNECTION_STRING\")\n",
        "SHARE_NAME = \"lqr\"\n",
        "BASE_DIR = \"success\"\n",
        "\n",
        "client = OpenAI(api_key=userdata.get(\"OPENAI_API_KEY\"))\n",
        "\n",
        "# --- Step 4: OpenAI stance classifier ---\n",
        "def classify_stance(headline, content):\n",
        "    prompt = f\"\"\"\n",
        "    You are analyzing a Telugu news article about YSRCP.\n",
        "    Decide the stance of the article toward YSRCP.\n",
        "\n",
        "    Rules:\n",
        "    - \"pro\" ‚Üí favorable to YSRCP or its leaders\n",
        "    - \"neutral\" ‚Üí factual reporting without visible bias\n",
        "    - \"anti\" ‚Üí critical or negative toward YSRCP or its leaders\n",
        "\n",
        "    Return ONLY a valid JSON object with:\n",
        "    - \"stance_label\": one of [\"pro\", \"neutral\", \"anti\"] (the highest probability class)\n",
        "    - \"stance_score\": object with keys \"pro\", \"neutral\", \"anti\" (each value is a float 0‚Äì1, all three must sum to 1)\n",
        "\n",
        "    Example output:\n",
        "    {{\n",
        "      \"stance_label\": \"anti\",\n",
        "      \"stance_score\": {{\n",
        "        \"pro\": 0.1,\n",
        "        \"neutral\": 0.2,\n",
        "        \"anti\": 0.7\n",
        "      }}\n",
        "    }}\n",
        "\n",
        "    Headline: {headline[:500]}\n",
        "    Article: {content[:2000]}\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a political news stance classifier. Return only valid JSON.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=0,\n",
        "            max_tokens=200\n",
        "        )\n",
        "\n",
        "        result_text = response.choices[0].message.content.strip()\n",
        "\n",
        "        # Clean possible code block wrappers\n",
        "        if result_text.startswith(\"```json\"):\n",
        "            result_text = result_text.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
        "\n",
        "        stance_data = json.loads(result_text)\n",
        "\n",
        "        # Validate structure\n",
        "        if \"stance_label\" not in stance_data or \"stance_score\" not in stance_data:\n",
        "            raise ValueError(\"Missing required fields in response\")\n",
        "\n",
        "        return stance_data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error in stance classification: {e}\")\n",
        "        return {\n",
        "            \"stance_label\": \"neutral\",\n",
        "            \"stance_score\": {\"pro\": 0.33, \"neutral\": 0.34, \"anti\": 0.33}\n",
        "        }\n",
        "\n",
        "# --- Step 5: Walk through all outlet folders ---\n",
        "try:\n",
        "    parent_dir_client = ShareDirectoryClient.from_connection_string(\n",
        "        conn_str=AZURE_CONN_STR,\n",
        "        share_name=SHARE_NAME,\n",
        "        directory_path=BASE_DIR\n",
        "    )\n",
        "\n",
        "    subdirs = list(parent_dir_client.list_directories_and_files())\n",
        "    print(f\"üìÅ Found {len(subdirs)} subdirectories\")\n",
        "\n",
        "    for subdir in subdirs:\n",
        "        if subdir.get(\"is_directory\", True):  # Only process directories\n",
        "            folder_name = subdir[\"name\"]   # Sakshi / Eenadu / future outlets\n",
        "            print(f\"\\nüîé Processing folder: {folder_name}\")\n",
        "\n",
        "            dir_client = ShareDirectoryClient.from_connection_string(\n",
        "                conn_str=AZURE_CONN_STR,\n",
        "                share_name=SHARE_NAME,\n",
        "                directory_path=f\"{BASE_DIR}/{folder_name}\"\n",
        "            )\n",
        "\n",
        "            files = list(dir_client.list_directories_and_files())\n",
        "            # Skip processed files and log files\n",
        "            json_files = [\n",
        "                f for f in files\n",
        "                if f[\"name\"].lower().endswith(\".json\")\n",
        "                and not f[\"name\"].startswith(\"processed_\")\n",
        "                and not f[\"name\"].lower().endswith(\"_log.json\")\n",
        "            ]\n",
        "\n",
        "            # --- Load or create log file for this folder ---\n",
        "            log_file_name = f\"{folder_name.lower()}_log.json\"\n",
        "            log_file_client = ShareFileClient.from_connection_string(\n",
        "                conn_str=AZURE_CONN_STR,\n",
        "                share_name=SHARE_NAME,\n",
        "                file_path=f\"{BASE_DIR}/{folder_name}/{log_file_name}\"\n",
        "            )\n",
        "\n",
        "            try:\n",
        "                log_data = log_file_client.download_file().readall().decode(\"utf-8\")\n",
        "                processed_log = json.loads(log_data)\n",
        "            except Exception:\n",
        "                processed_log = []  # create new log if missing\n",
        "\n",
        "            print(f\"üìÑ Found {len(json_files)} JSON files to check. {len(processed_log)} already processed.\")\n",
        "\n",
        "            for f in json_files:\n",
        "                if f[\"name\"] in processed_log:\n",
        "                    print(f\"‚è≠Ô∏è Skipping {f['name']} (already in log)\")\n",
        "                    continue\n",
        "\n",
        "                print(f\"\\nüìÑ Processing file: {f['name']}\")\n",
        "\n",
        "                try:\n",
        "                    # --- Download the JSON file ---\n",
        "                    file_client = ShareFileClient.from_connection_string(\n",
        "                        conn_str=AZURE_CONN_STR,\n",
        "                        share_name=SHARE_NAME,\n",
        "                        file_path=f\"{BASE_DIR}/{folder_name}/{f['name']}\"\n",
        "                    )\n",
        "                    downloaded = file_client.download_file().readall()\n",
        "                    data = json.loads(downloaded.decode(\"utf-8\"))\n",
        "\n",
        "                    # --- Extract content (handle embedded JSON from Gemini) ---\n",
        "                    raw_content = data.get(\"content\", \"\")\n",
        "                    if \"```json\" in raw_content:\n",
        "                        try:\n",
        "                            json_start = raw_content.find(\"```json\") + 7\n",
        "                            json_end = raw_content.find(\"```\", json_start)\n",
        "                            json_str = raw_content[json_start:json_end].strip()\n",
        "                            embedded_data = json.loads(json_str)\n",
        "                            headline = embedded_data.get(\"headline\", \"\")\n",
        "                            content = embedded_data.get(\"content\", \"\")\n",
        "                        except Exception as e:\n",
        "                            print(f\"‚ö†Ô∏è Error parsing embedded JSON: {e}\")\n",
        "                            headline = data.get(\"headline\", \"\")\n",
        "                            content = raw_content\n",
        "                    else:\n",
        "                        headline = data.get(\"headline\", \"\")\n",
        "                        content = raw_content\n",
        "\n",
        "                    source = folder_name   # e.g. Sakshi / Eenadu\n",
        "\n",
        "                    print(f\"üì∞ Headline: {headline[:100]}...\")\n",
        "\n",
        "                    # --- Run stance classification ---\n",
        "                    stance_result = classify_stance(headline, content)\n",
        "\n",
        "                    # --- Build enriched JSON ---\n",
        "                    enriched = {\n",
        "                        \"id\": f\"{source}-{datetime.now().strftime('%Y%m%d%H%M%S')}\",\n",
        "                        \"source\": source,\n",
        "                        \"source_bias\": \"pro-YSRCP\" if source.lower() == \"sakshi\" else \"anti-YSRCP\" if source.lower() == \"eenadu\" else \"neutral\",\n",
        "                        \"type\": \"news_article\",\n",
        "                        \"date\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
        "                        \"headline\": headline,\n",
        "                        \"content\": content,\n",
        "                        \"stance_label\": stance_result[\"stance_label\"],\n",
        "                        \"stance_score\": stance_result[\"stance_score\"]\n",
        "                    }\n",
        "\n",
        "                    # --- Save enriched file with overwrite ---\n",
        "                    new_name = f\"processed_{f['name']}\"\n",
        "                    new_file_client = ShareFileClient.from_connection_string(\n",
        "                        conn_str=AZURE_CONN_STR,\n",
        "                        share_name=SHARE_NAME,\n",
        "                        file_path=f\"{BASE_DIR}/{folder_name}/{new_name}\"\n",
        "                    )\n",
        "                    new_content = json.dumps(enriched, ensure_ascii=False, indent=2).encode(\"utf-8\")\n",
        "\n",
        "                    # Create or overwrite file\n",
        "                    try:\n",
        "                        new_file_client.create_file(size=len(new_content))\n",
        "                    except Exception:\n",
        "                        new_file_client.delete_file()\n",
        "                        new_file_client.create_file(size=len(new_content))\n",
        "\n",
        "                    new_file_client.upload_file(new_content)\n",
        "\n",
        "                    print(f\"‚úÖ Saved enriched file: {new_name}\")\n",
        "                    print(f\"üè∑Ô∏è Stance: {stance_result['stance_label']} (confidence: {max(stance_result['stance_score'].values()):.2f})\")\n",
        "\n",
        "                    # --- Update log file ---\n",
        "                    processed_log.append(f[\"name\"])\n",
        "                    updated_log = json.dumps(processed_log, indent=2, ensure_ascii=False).encode(\"utf-8\")\n",
        "                    try:\n",
        "                        log_file_client.create_file(size=len(updated_log))\n",
        "                    except Exception:\n",
        "                        log_file_client.delete_file()\n",
        "                        log_file_client.create_file(size=len(updated_log))\n",
        "                    log_file_client.upload_file(updated_log)\n",
        "                    print(f\"üìù Log updated: {log_file_name}\")\n",
        "\n",
        "                except Exception as file_error:\n",
        "                    print(f\"‚ùå Error processing file {f['name']}: {file_error}\")\n",
        "                    continue\n",
        "\n",
        "except Exception as main_error:\n",
        "    print(f\"‚ùå Main execution error: {main_error}\")\n",
        "\n",
        "print(\"\\nüéâ Processing complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "em6zKCGLpzSe",
        "outputId": "8dcc80cd-9062-4a4d-ff1b-17abe71da677"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: azure-storage-file-share in /usr/local/lib/python3.12/dist-packages (12.22.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.108.0)\n",
            "Requirement already satisfied: azure-core>=1.30.0 in /usr/local/lib/python3.12/dist-packages (from azure-storage-file-share) (1.35.1)\n",
            "Requirement already satisfied: cryptography>=2.1.4 in /usr/local/lib/python3.12/dist-packages (from azure-storage-file-share) (43.0.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from azure-storage-file-share) (4.15.0)\n",
            "Requirement already satisfied: isodate>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from azure-storage-file-share) (0.7.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.9)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from azure-core>=1.30.0->azure-storage-file-share) (2.32.4)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from azure-core>=1.30.0->azure-storage-file-share) (1.17.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=2.1.4->azure-storage-file-share) (2.0.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-file-share) (2.23)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-file-share) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-file-share) (2.5.0)\n",
            "üìÅ Found 3 subdirectories\n",
            "\n",
            "üîé Processing folder: Eenadu\n",
            "üìÑ Found 9 JSON files to check. 9 already processed.\n",
            "‚è≠Ô∏è Skipping ACB Court Madhyeam Scam Niditulaku Noticelu ee09092025.json (already in log)\n",
            "‚è≠Ô∏è Skipping Annam Venkatramana reddy Madhyeam Scam Raj kesireddy Saireddy ee 07082025.json (already in log)\n",
            "‚è≠Ô∏è Skipping babu Cabineat Meeting Comments ee21052025.json (already in log)\n",
            "‚è≠Ô∏è Skipping babu Madhyeam Scam Charge Sheet Vesaka YSRCP Leaders ni Endagtadi ee 19072025.json (already in log)\n",
            "‚è≠Ô∏è Skipping Bail Upasamrishukuna Vasudevareddy ee 03072024.json (already in log)\n",
            "‚è≠Ô∏è Skipping Bevarejash Corporation EX MD Vasudevareddy ki Bail ee30112024.json (already in log)\n",
            "‚è≠Ô∏è Skipping Bevarejesh Corporation EX Md Vasudevareddy CID Case ee 08062024.json (already in log)\n",
            "‚è≠Ô∏è Skipping Bevarejus Corporation Vasudevareddy pi Vetu ee 17042024.json (already in log)\n",
            "‚è≠Ô∏è Skipping BJP MP CM Ramesh YCP hayeam 30vela Cores Madhyeam Scam Liquar ee 12022025.json (already in log)\n",
            "\n",
            "üîé Processing folder: sakshi\n",
            "üìÑ Found 10 JSON files to check. 10 already processed.\n",
            "‚è≠Ô∏è Skipping 12na SIt Vicharana ku Vijyasaireddy Madhyeam Scam lo sak 11072025.json (already in log)\n",
            "‚è≠Ô∏è Skipping babu Madhyeam Scam pi Evaru Matldodhu ska 21052025.json (already in log)\n",
            "‚è≠Ô∏è Skipping Balaji Govindhapa arrest sak1405025.json (already in log)\n",
            "‚è≠Ô∏è Skipping Balaji Govindhapa Naa arrest Akaramam sak 15052025.json (already in log)\n",
            "‚è≠Ô∏è Skipping Bevarejash Corporation Vasudevareddy babu GVT Vedimpulu ska 08012025.json (already in log)\n",
            "‚è≠Ô∏è Skipping Bevarejes EX MD Vasudevareddy pi Tapudu Stament Ivakunte Irikistam Vijyasaireddy Mithenreddy pi caselu petali  sak 17122024.json (already in log)\n",
            "‚è≠Ô∏è Skipping Bevarejesh EX MD Vasudevareddy ki CID Tapudu Statement Ivalani Otilu sak 20012025.json (already in log)\n",
            "‚è≠Ô∏è Skipping Bevaresh Vasudevareddy pi CID CAse ska 08062024.json (already in log)\n",
            "‚è≠Ô∏è Skipping Chevireddy Bail Petishions Vicharana Voiedha Madhyema Scam Sak11072025.json (already in log)\n",
            "‚è≠Ô∏è Skipping Chevireddy ki Aswstata sak 22062025.json (already in log)\n",
            "\n",
            "üéâ Processing complete!\n"
          ]
        }
      ]
    }
  ]
}